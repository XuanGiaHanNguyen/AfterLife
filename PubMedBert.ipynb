{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPOKxs9jxGeyjVeah+OqHu9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/XuanGiaHanNguyen/AfterLife/blob/main/PubMedBert.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TSk4Mn_K1yor",
        "outputId": "23513db3-e085-4e34-abea-54e693fdce82"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers==4.40.0 in /usr/local/lib/python3.12/dist-packages (4.40.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers==4.40.0) (3.21.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.12/dist-packages (from transformers==4.40.0) (0.36.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers==4.40.0) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers==4.40.0) (26.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers==4.40.0) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers==4.40.0) (2025.11.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers==4.40.0) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.12/dist-packages (from transformers==4.40.0) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.12/dist-packages (from transformers==4.40.0) (0.7.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers==4.40.0) (4.67.3)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.40.0) (2025.3.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.40.0) (1.2.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.40.0) (4.15.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.40.0) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.40.0) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.40.0) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.40.0) (2026.1.4)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (2.14.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (2.32.4)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.12/dist-packages (from datasets) (4.67.3)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets) (3.6.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.12/dist-packages (from datasets) (0.70.15)\n",
            "Requirement already satisfied: fsspec>=2021.11.1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]>=2021.11.1->datasets) (2025.3.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.12/dist-packages (from datasets) (3.13.3)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.14.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.36.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from datasets) (26.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from datasets) (6.0.3)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasets) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasets) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasets) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasets) (6.7.1)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasets) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasets) (1.22.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (3.21.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (1.2.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (4.15.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->datasets) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->datasets) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->datasets) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->datasets) (2026.1.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Looking in indexes: https://download.pytorch.org/whl/cu118\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.9.0+cu128)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (0.24.0+cu128)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.12/dist-packages (2.9.0+cu128)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.21.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch) (12.8.93)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch) (12.8.90)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch) (12.8.90)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.8.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.3.83)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.9.90)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.3.90)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.8.93)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch) (12.8.90)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch) (12.8.93)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.1.3)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.5.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision) (11.3.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n",
            "Requirement already satisfied: seqeval in /usr/local/lib/python3.12/dist-packages (1.2.2)\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.12/dist-packages (from seqeval) (2.0.2)\n",
            "Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.12/dist-packages (from seqeval) (1.6.1)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.16.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.5.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=0.21.3->seqeval) (3.6.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.16.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.12/dist-packages (1.12.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from accelerate) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from accelerate) (26.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from accelerate) (6.0.3)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from accelerate) (2.9.0+cu128)\n",
            "Requirement already satisfied: huggingface_hub>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from accelerate) (0.36.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from accelerate) (0.7.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.0->accelerate) (3.21.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.0->accelerate) (2025.3.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.0->accelerate) (1.2.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.0->accelerate) (2.32.4)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.0->accelerate) (4.67.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.0->accelerate) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.8.93)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.8.90)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.8.90)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.8.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (11.3.3.83)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (10.3.9.90)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (11.7.3.90)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.5.8.93)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.8.90)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.8.93)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (1.13.1.3)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.5.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub>=0.21.0->accelerate) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub>=0.21.0->accelerate) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub>=0.21.0->accelerate) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub>=0.21.0->accelerate) (2026.1.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (4.67.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "!pip3 install transformers==4.40.0\n",
        "!pip3 install datasets\n",
        "!pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
        "!pip3 install seqeval          # for NER evaluation (entity-level F1)\n",
        "!pip3 install scikit-learn\n",
        "!pip3 install accelerate\n",
        "!pip3 install numpy pandas tqdm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "print(torch.cuda.is_available())       # Should print True\n",
        "print(torch.cuda.get_device_name(0))   # Shows your GPU name"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Y-LEMnd2lTr",
        "outputId": "a8d76a3c-6358-490b-8f9e-8209e92a1e75"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n",
            "Tesla T4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModel\n",
        "\n",
        "model_name = \"microsoft/BiomedNLP-BiomedBERT-base-uncased-abstract\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModel.from_pretrained(model_name)\n",
        "\n",
        "print(\"Model loaded successfully!\")\n",
        "print(f\"Parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
        "# Should print ~110 million parameters"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wmu0E89Y2qNb",
        "outputId": "75f4f904-696d-4c6a-db67-77790bf274c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py:949: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model loaded successfully!\n",
            "Parameters: 109,482,240\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**NER test:**\n",
        "\n"
      ],
      "metadata": {
        "id": "tJLxE-OwEju7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tried many data-set download, but failed so have to use kaggle"
      ],
      "metadata": {
        "id": "6y-ai4KI5FbN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import kagglehub\n",
        "\n",
        "# Download JNLPBA - actual BLURB benchmark dataset\n",
        "path = kagglehub.dataset_download(\"athibant/jnlpba\")\n",
        "print(\"Path:\", path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O4REX-Ni6BPf",
        "outputId": "71b0fa7b-b369-40bd-a736-b3f8ed658da0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.13), please consider upgrading to the latest version (1.0.0).\n",
            "Downloading from https://www.kaggle.com/api/v1/datasets/download/athibant/jnlpba?dataset_version_number=2...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1.36M/1.36M [00:01<00:00, 1.34MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting files...\n",
            "Path: /root/.cache/kagglehub/datasets/athibant/jnlpba/versions/2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "for f in os.listdir(path):\n",
        "    print(f)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X8Saeexb6Gzw",
        "outputId": "9f2d7994-e274-4b34-c728-27d052caaab5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "JNLPBA\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "See in the files"
      ],
      "metadata": {
        "id": "0P9jbu7P6Kko"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "for f in os.listdir(path):\n",
        "    print(f)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wAKmSbhL6L86",
        "outputId": "8cecb7f2-6c70-4283-a3e7-9a79513c0c43"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "JNLPBA\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for root, dirs, files in os.walk(path):\n",
        "    for f in files:\n",
        "        print(os.path.join(root, f))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nHzA4CkW6S8i",
        "outputId": "a14fa13a-b43e-4968-d61b-3c717ca6ab59"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/root/.cache/kagglehub/datasets/athibant/jnlpba/versions/2/JNLPBA/devel.tsv\n",
            "/root/.cache/kagglehub/datasets/athibant/jnlpba/versions/2/JNLPBA/test.tsv\n",
            "/root/.cache/kagglehub/datasets/athibant/jnlpba/versions/2/JNLPBA/train.tsv\n",
            "/root/.cache/kagglehub/datasets/athibant/jnlpba/versions/2/JNLPBA/classes.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Load and inspect data"
      ],
      "metadata": {
        "id": "aI5Kevcu6Yfe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_conll(filepath):\n",
        "    sentences, labels = [], []\n",
        "    tokens, tags = [], []\n",
        "    with open(filepath) as f:\n",
        "        for line in f:\n",
        "            line = line.strip()\n",
        "            if line == \"\":\n",
        "                if tokens:\n",
        "                    sentences.append(tokens)\n",
        "                    labels.append(tags)\n",
        "                    tokens, tags = [], []\n",
        "            else:\n",
        "                parts = line.split(\"\\t\")\n",
        "                tokens.append(parts[0])\n",
        "                tags.append(parts[-1])\n",
        "    return sentences, labels\n",
        "\n",
        "base = \"/root/.cache/kagglehub/datasets/athibant/jnlpba/versions/2/JNLPBA\"\n",
        "\n",
        "train_sents, train_labels = load_conll(f\"{base}/train.tsv\")\n",
        "dev_sents,   dev_labels   = load_conll(f\"{base}/devel.tsv\")\n",
        "test_sents,  test_labels  = load_conll(f\"{base}/test.tsv\")\n",
        "\n",
        "print(f\"Train: {len(train_sents)} sentences\")\n",
        "print(f\"Dev:   {len(dev_sents)} sentences\")\n",
        "print(f\"Test:  {len(test_sents)} sentences\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xcs62bWZ6aKy",
        "outputId": "e8de1efb-d9ae-452a-8f0f-d4734cd5f0e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: 18607 sentences\n",
            "Dev:   1939 sentences\n",
            "Test:  4260 sentences\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preview and check lable"
      ],
      "metadata": {
        "id": "WmC0WOd16Xwm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Sample tokens:\", train_sents[0])\n",
        "print(\"Sample labels:\", train_labels[0])\n",
        "\n",
        "unique_labels = sorted(set(l for labels in train_labels for l in labels))\n",
        "print(\"\\nUnique labels:\", unique_labels)\n",
        "print(\"Total label types:\", len(unique_labels))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bHQQMmEJ6ewh",
        "outputId": "bd5291b0-7478-4731-b0d4-61c6e43eb704"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample tokens: ['-DOCSTART-']\n",
            "Sample labels: ['O']\n",
            "\n",
            "Unique labels: ['B-DNA', 'B-RNA', 'B-cell_line', 'B-cell_type', 'B-protein', 'I-DNA', 'I-RNA', 'I-cell_line', 'I-cell_type', 'I-protein', 'O']\n",
            "Total label types: 11\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Building full pipeline"
      ],
      "metadata": {
        "id": "g-cFLQjC6lqb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "unique_labels = ['B-DNA', 'B-RNA', 'B-cell_line', 'B-cell_type', 'B-protein',\n",
        "                 'I-DNA', 'I-RNA', 'I-cell_line', 'I-cell_type', 'I-protein', 'O']\n",
        "\n",
        "label2id = {l: i for i, l in enumerate(unique_labels)}\n",
        "id2label = {i: l for i, l in enumerate(unique_labels)}\n",
        "\n",
        "print(\"Label mappings ready:\", label2id)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eS2lvNMX6na7",
        "outputId": "3246531d-e510-4e4a-b804-f9c807089b96"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Label mappings ready: {'B-DNA': 0, 'B-RNA': 1, 'B-cell_line': 2, 'B-cell_type': 3, 'B-protein': 4, 'I-DNA': 5, 'I-RNA': 6, 'I-cell_line': 7, 'I-cell_type': 8, 'I-protein': 9, 'O': 10}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Building dataset class"
      ],
      "metadata": {
        "id": "LSOnXiXl6qEf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/BiomedNLP-BiomedBERT-base-uncased-abstract\")\n",
        "\n",
        "class NERDataset(Dataset):\n",
        "    def __init__(self, sentences, labels, tokenizer, max_len=128):\n",
        "        self.sentences = sentences\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sentences)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        tokens = self.sentences[idx]\n",
        "        labels = self.labels[idx]\n",
        "\n",
        "        encoding = self.tokenizer(\n",
        "            tokens,\n",
        "            is_split_into_words=True,\n",
        "            max_length=self.max_len,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        # Align labels to subword tokens\n",
        "        word_ids = encoding.word_ids()\n",
        "        aligned_labels = []\n",
        "        prev_word_id = None\n",
        "        for word_id in word_ids:\n",
        "            if word_id is None:\n",
        "                aligned_labels.append(-100)  # special tokens → ignore\n",
        "            elif word_id != prev_word_id:\n",
        "                aligned_labels.append(label2id[labels[word_id]])\n",
        "            else:\n",
        "                aligned_labels.append(-100)  # subword continuation → ignore\n",
        "            prev_word_id = word_id\n",
        "\n",
        "        return {\n",
        "            'input_ids':      encoding['input_ids'].squeeze(),\n",
        "            'attention_mask': encoding['attention_mask'].squeeze(),\n",
        "            'token_type_ids': encoding['token_type_ids'].squeeze(),\n",
        "            'labels':         torch.tensor(aligned_labels)\n",
        "        }\n",
        "\n",
        "# Filter out -DOCSTART- sentences\n",
        "def filter_docstart(sents, labs):\n",
        "    return zip(*[(s, l) for s, l in zip(sents, labs) if s != ['-DOCSTART-']])\n",
        "\n",
        "tr_s, tr_l = filter_docstart(train_sents, train_labels)\n",
        "dv_s, dv_l = filter_docstart(dev_sents,   dev_labels)\n",
        "te_s, te_l = filter_docstart(test_sents,  test_labels)\n",
        "\n",
        "train_dataset = NERDataset(list(tr_s), list(tr_l), tokenizer)\n",
        "dev_dataset   = NERDataset(list(dv_s), list(dv_l), tokenizer)\n",
        "test_dataset  = NERDataset(list(te_s), list(te_l), tokenizer)\n",
        "\n",
        "print(f\"Train: {len(train_dataset)} | Dev: {len(dev_dataset)} | Test: {len(test_dataset)}\")\n",
        "print(\"Sample input_ids shape:\", train_dataset[0]['input_ids'].shape)\n",
        "print(\"Sample labels shape:\",    train_dataset[0]['labels'].shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hsO3v-_86pdM",
        "outputId": "512e6db4-6fef-44f8-d7d2-bfb3c2d5500b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py:949: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: 16807 | Dev: 1739 | Test: 3856\n",
            "Sample input_ids shape: torch.Size([128])\n",
            "Sample labels shape: torch.Size([128])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loading the model"
      ],
      "metadata": {
        "id": "zQ9hEGA46vSU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForTokenClassification\n",
        "\n",
        "model = AutoModelForTokenClassification.from_pretrained(\n",
        "    \"microsoft/BiomedNLP-BiomedBERT-base-uncased-abstract\",\n",
        "    num_labels=len(unique_labels),\n",
        "    id2label=id2label,\n",
        "    label2id=label2id\n",
        ")\n",
        "model = model.cuda()\n",
        "print(\"Model loaded on GPU\")\n",
        "print(f\"Output labels: {model.config.num_labels}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sDENAs866wf8",
        "outputId": "5874370b-bbb3-4d7f-bbef-29ff5ea3682e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForTokenClassification were not initialized from the model checkpoint at microsoft/BiomedNLP-BiomedBERT-base-uncased-abstract and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model loaded on GPU\n",
            "Output labels: 11\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training loop"
      ],
      "metadata": {
        "id": "6qbsJYfc664R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "from transformers import AdamW, get_linear_schedule_with_warmup\n",
        "from seqeval.metrics import f1_score, classification_report\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Hyperparameters (matching the paper)\n",
        "EPOCHS     = 5\n",
        "LR         = 3e-5\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "dev_loader   = DataLoader(dev_dataset,   batch_size=BATCH_SIZE)\n",
        "test_loader  = DataLoader(test_dataset,  batch_size=BATCH_SIZE)\n",
        "\n",
        "optimizer = AdamW(model.parameters(), lr=LR, weight_decay=0.01)\n",
        "total_steps = len(train_loader) * EPOCHS\n",
        "scheduler = get_linear_schedule_with_warmup(\n",
        "    optimizer,\n",
        "    num_warmup_steps=int(0.1 * total_steps),\n",
        "    num_training_steps=total_steps\n",
        ")\n",
        "\n",
        "def evaluate(model, loader):\n",
        "    model.eval()\n",
        "    all_preds, all_labels = [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in loader:\n",
        "            input_ids      = batch['input_ids'].cuda()\n",
        "            attention_mask = batch['attention_mask'].cuda()\n",
        "            token_type_ids = batch['token_type_ids'].cuda()\n",
        "            labels         = batch['labels'].cuda()\n",
        "\n",
        "            outputs = model(input_ids=input_ids,\n",
        "                           attention_mask=attention_mask,\n",
        "                           token_type_ids=token_type_ids)\n",
        "\n",
        "            preds = torch.argmax(outputs.logits, dim=-1)\n",
        "\n",
        "            for pred_seq, label_seq in zip(preds, labels):\n",
        "                pred_list, label_list = [], []\n",
        "                for p, l in zip(pred_seq, label_seq):\n",
        "                    if l.item() == -100:\n",
        "                        continue\n",
        "                    pred_list.append(id2label[p.item()])\n",
        "                    label_list.append(id2label[l.item()])\n",
        "                all_preds.append(pred_list)\n",
        "                all_labels.append(label_list)\n",
        "\n",
        "    f1 = f1_score(all_labels, all_preds)\n",
        "    return f1, all_preds, all_labels\n",
        "\n",
        "# Training\n",
        "best_dev_f1  = 0\n",
        "best_epoch   = 0\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{EPOCHS}\"):\n",
        "        input_ids      = batch['input_ids'].cuda()\n",
        "        attention_mask = batch['attention_mask'].cuda()\n",
        "        token_type_ids = batch['token_type_ids'].cuda()\n",
        "        labels         = batch['labels'].cuda()\n",
        "\n",
        "        outputs = model(input_ids=input_ids,\n",
        "                       attention_mask=attention_mask,\n",
        "                       token_type_ids=token_type_ids,\n",
        "                       labels=labels)\n",
        "\n",
        "        loss = outputs.loss\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "    avg_loss = total_loss / len(train_loader)\n",
        "    dev_f1, _, _ = evaluate(model, dev_loader)\n",
        "\n",
        "    print(f\"Epoch {epoch+1} | Loss: {avg_loss:.4f} | Dev F1: {dev_f1:.4f}\")\n",
        "\n",
        "    if dev_f1 > best_dev_f1:\n",
        "        best_dev_f1 = dev_f1\n",
        "        best_epoch  = epoch + 1\n",
        "        torch.save(model.state_dict(), \"best_model.pt\")\n",
        "        print(f\"  ✅ New best saved!\")\n",
        "\n",
        "print(f\"\\nBest Dev F1: {best_dev_f1:.4f} at epoch {best_epoch}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1aRrB4XW68Qk",
        "outputId": "dc3a96c8-d7d4-44bf-bd63-8abd4506ce53"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/transformers/optimization.py:521: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "Epoch 1/5: 100%|██████████| 526/526 [05:48<00:00,  1.51it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1 | Loss: 0.4145 | Dev F1: 0.7748\n",
            "  ✅ New best saved!\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2/5: 100%|██████████| 526/526 [05:53<00:00,  1.49it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2 | Loss: 0.1382 | Dev F1: 0.7864\n",
            "  ✅ New best saved!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/5: 100%|██████████| 526/526 [05:54<00:00,  1.49it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3 | Loss: 0.1106 | Dev F1: 0.7864\n",
            "  ✅ New best saved!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4/5: 100%|██████████| 526/526 [05:53<00:00,  1.49it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4 | Loss: 0.0898 | Dev F1: 0.7896\n",
            "  ✅ New best saved!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5/5: 100%|██████████| 526/526 [05:54<00:00,  1.49it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5 | Loss: 0.0758 | Dev F1: 0.7907\n",
            "  ✅ New best saved!\n",
            "\n",
            "Best Dev F1: 0.7907 at epoch 5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from seqeval.metrics import classification_report\n",
        "\n",
        "# Load best model\n",
        "model.load_state_dict(torch.load(\"best_model.pt\"))\n",
        "\n",
        "test_f1, test_preds, test_labels_list = evaluate(model, test_loader)\n",
        "print(f\"Test F1: {test_f1:.4f}\")\n",
        "print(f\"Paper target: 0.7910\")\n",
        "print(f\"Gap: {test_f1 - 0.7910:+.4f}\")\n",
        "print()\n",
        "print(classification_report(test_labels_list, test_preds))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PdRKvoKECT5y",
        "outputId": "c858ec4f-b87f-40f0-86ec-a3c5e7f7450a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test F1: 0.7488\n",
            "Paper target: 0.7910\n",
            "Gap: -0.0422\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         DNA       0.70      0.76      0.73      1054\n",
            "         RNA       0.70      0.76      0.73       118\n",
            "   cell_line       0.52      0.72      0.61       500\n",
            "   cell_type       0.77      0.73      0.75      1920\n",
            "     protein       0.71      0.84      0.77      5064\n",
            "\n",
            "   micro avg       0.70      0.80      0.75      8656\n",
            "   macro avg       0.68      0.76      0.72      8656\n",
            "weighted avg       0.71      0.80      0.75      8656\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**PICO Extraction**"
      ],
      "metadata": {
        "id": "wrGytqTfEn06"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Downloading EBM-NLP from Github\n"
      ],
      "metadata": {
        "id": "7xyaDh9MEr4y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/bepnye/EBM-NLP.git\n",
        "!ls EBM-NLP/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tqKptGvgG9XI",
        "outputId": "e7a70f56-5ca4-43a3-b93f-92af0614ba6a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'EBM-NLP'...\n",
            "remote: Enumerating objects: 200, done.\u001b[K\n",
            "remote: Total 200 (delta 0), reused 0 (delta 0), pack-reused 200 (from 1)\u001b[K\n",
            "Receiving objects: 100% (200/200), 83.47 MiB | 11.43 MiB/s, done.\n",
            "Resolving deltas: 100% (82/82), done.\n",
            "ebm_nlp_1_00.tar.gz  ebm_nlp_difficulty.tar.gz\tphase1_example.png\n",
            "ebm_nlp_2_00.tar.gz  generate_bio_labels.py\tREADME.md\n",
            "ebm_nlp_demo.py      models\t\t\tREADME.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for root, dirs, files in os.walk(\"EBM-NLP\"):\n",
        "    depth = root.replace(\"EBM-NLP\", '').count(os.sep)\n",
        "    if depth < 4:\n",
        "        indent = '  ' * depth\n",
        "        print(f\"{indent}{os.path.basename(root)}/\")\n",
        "        for f in files[:3]:\n",
        "            print(f\"{indent}  {f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mFujIwBKHC4B",
        "outputId": "ea9995c9-ed30-4fbf-b3f7-85632f50fda8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EBM-NLP/\n",
            "  .gitattributes\n",
            "  ebm_nlp_difficulty.tar.gz\n",
            "  ebm_nlp_1_00.tar.gz\n",
            "  .git/\n",
            "    HEAD\n",
            "    description\n",
            "    index\n",
            "    hooks/\n",
            "      pre-push.sample\n",
            "      pre-receive.sample\n",
            "      prepare-commit-msg.sample\n",
            "    info/\n",
            "      exclude\n",
            "    logs/\n",
            "      HEAD\n",
            "      refs/\n",
            "    objects/\n",
            "      info/\n",
            "      pack/\n",
            "        pack-3b810602da3c8d1e11e6fb0a35b78e76f36a3dae.pack\n",
            "        pack-3b810602da3c8d1e11e6fb0a35b78e76f36a3dae.idx\n",
            "    branches/\n",
            "    refs/\n",
            "      heads/\n",
            "        master\n",
            "      remotes/\n",
            "      tags/\n",
            "  models/\n",
            "    eval.py\n",
            "    logreg/\n",
            "      logreg.py\n",
            "    lstm-crf/\n",
            "      LICENSE.txt\n",
            "      requirements.txt\n",
            "      makefile\n",
            "      model/\n",
            "        config.py\n",
            "        base_model.py\n",
            "        general_utils.py\n",
            "    crf/\n",
            "      p1/\n",
            "        cr-v.sh\n",
            "        model.py\n",
            "      p2/\n",
            "        crf.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The actual data is in the tar.gz files. Let's extract them:"
      ],
      "metadata": {
        "id": "g7hZ0CJtHLde"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!tar -xzf EBM-NLP/ebm_nlp_1_00.tar.gz -C EBM-NLP/\n",
        "!ls EBM-NLP/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GGgpktVFHL-0",
        "outputId": "342606f7-3577-4f40-f0c2-327f76055d4d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ebm_nlp_1_00\t     ebm_nlp_demo.py\t\tmodels\t\t    README.txt\n",
            "ebm_nlp_1_00.tar.gz  ebm_nlp_difficulty.tar.gz\tphase1_example.png\n",
            "ebm_nlp_2_00.tar.gz  generate_bio_labels.py\tREADME.md\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for root, dirs, files in os.walk(\"EBM-NLP/ebm_nlp_1_00\"):\n",
        "    depth = root.replace(\"EBM-NLP/ebm_nlp_1_00\", '').count(os.sep)\n",
        "    if depth < 4:\n",
        "        indent = '  ' * depth\n",
        "        print(f\"{indent}{os.path.basename(root)}/\")\n",
        "        for f in files[:3]:\n",
        "            print(f\"{indent}  {f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j-VuTQvvHWEJ",
        "outputId": "55e81291-acf6-4ecd-d11d-3595c44a7630"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ebm_nlp_1_00/\n",
            "  ._.DS_Store\n",
            "  README.txt\n",
            "  .DS_Store\n",
            "  annotations/\n",
            "    ._.DS_Store\n",
            "    .DS_Store\n",
            "    aggregated/\n",
            "      ._.DS_Store\n",
            "      .DS_Store\n",
            "      starting_spans/\n",
            "        ._.DS_Store\n",
            "        .DS_Store\n",
            "      hierarchical_labels/\n",
            "    individual/\n",
            "      ._.DS_Store\n",
            "      .DS_Store\n",
            "      starting_spans/\n",
            "        ._.DS_Store\n",
            "        .DS_Store\n",
            "      hierarchical_labels/\n",
            "  documents/\n",
            "    25313065.tokens\n",
            "    12486433.tokens\n",
            "    22972771.tokens\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "base2 = \"EBM-NLP/ebm_nlp_1_00\"\n",
        "\n",
        "# Check what's in aggregated/starting_spans\n",
        "ann_path = f\"{base2}/annotations/aggregated/starting_spans\"\n",
        "for item in os.listdir(ann_path):\n",
        "    print(item)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M_Ed5NteHvFr",
        "outputId": "6b22cb74-a92c-4c77-9b2d-d771b866680b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "._.DS_Store\n",
            "outcomes\n",
            "interventions\n",
            "participants\n",
            ".DS_Store\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# See subfolders\n",
        "for folder in os.listdir(ann_path):\n",
        "    subpath = f\"{ann_path}/{folder}\"\n",
        "    if os.path.isdir(subpath):\n",
        "        files = os.listdir(subpath)\n",
        "        print(f\"{folder}/: {files[:3]}\")\n",
        "\n",
        "# Read one sample annotation\n",
        "import glob\n",
        "sample_ann = glob.glob(f\"{ann_path}/**/*.ann\", recursive=True)[0]\n",
        "print(f\"\\nFile: {sample_ann}\")\n",
        "with open(sample_ann) as f:\n",
        "    print(f.read()[:300])\n",
        "\n",
        "# Read matching token file\n",
        "doc_id = os.path.basename(sample_ann).replace(\".ann\", \"\")\n",
        "with open(f\"{base2}/documents/{doc_id}.tokens\") as f:\n",
        "    print(f\"\\nTokens:\\n{f.read()[:300]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 325
        },
        "id": "BGRTsa8bHxP9",
        "outputId": "4309fa31-221f-4195-bcfa-7bc2f04a86d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "outcomes/: ['train', 'test']\n",
            "interventions/: ['._.DS_Store', 'train', '.DS_Store']\n",
            "participants/: ['train', 'test']\n",
            "\n",
            "File: EBM-NLP/ebm_nlp_1_00/annotations/aggregated/starting_spans/outcomes/train/26449739_AGGREGATED.ann\n",
            "0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'EBM-NLP/ebm_nlp_1_00/documents/26449739_AGGREGATED.tokens'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-838829761.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m# Read matching token file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mdoc_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbasename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_ann\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".ann\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{base2}/documents/{doc_id}.tokens\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"\\nTokens:\\n{f.read()[:300]}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'EBM-NLP/ebm_nlp_1_00/documents/26449739_AGGREGATED.tokens'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The annotation format is clear — it's a comma-separated list of 0s and 1s, one per token. The filename just needs the _AGGREGATED stripped. Let's fix and build the full loader:"
      ],
      "metadata": {
        "id": "a4vbtBGwH8zA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "label_map = {\"participants\": \"P\", \"interventions\": \"I\", \"outcomes\": \"O\"}\n",
        "\n",
        "# Check first few docs\n",
        "count = 0\n",
        "for doc_id in sorted(os.listdir(f\"{ann_base}/participants/train\"))[:5]:\n",
        "    doc_id = doc_id.replace(\"_AGGREGATED.ann\", \"\")\n",
        "    tok_file = f\"{doc_base}/{doc_id}.tokens\"\n",
        "\n",
        "    if not os.path.exists(tok_file):\n",
        "        print(f\"{doc_id}: token file missing\")\n",
        "        continue\n",
        "\n",
        "    with open(tok_file) as f:\n",
        "        tokens = [t.strip() for t in f.readlines() if t.strip()]\n",
        "\n",
        "    ann_file = f\"{ann_base}/participants/train/{doc_id}_AGGREGATED.ann\"\n",
        "    with open(ann_file) as f:\n",
        "        vals = [int(x) for x in f.read().strip().split(\",\") if x.strip()]\n",
        "\n",
        "    print(f\"{doc_id}: tokens={len(tokens)}, ann_vals={len(vals)}, match={len(tokens)==len(vals)}\")\n",
        "    print(f\"  First 5 tokens: {tokens[:5]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9G6p6CGvIWGS",
        "outputId": "919f1654-bde5-4192-d741-46a361204c50"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10036953: tokens=1, ann_vals=162, match=False\n",
            "  First 5 tokens: ['[ Triple therapy regimens involving H2 blockaders for therapy of Helicobacter pylori infections ] . Comparison of ranitidine and lansoprazole in short-term low-dose triple therapy for Helicobacter pylori infection . To evaluate the efficacy and safety of two 1-week low-dose triple-therapy drug regimens involving antisecretory drugs for Helicobacter pylori infection , 99 patients with H. pylori infection were treated with either lansoprazole ( LPZ ) or ranitidine ( RNT ) used together with clarithromycin ( CAM ) and metrinidazole ( MTZ ) . The drug combination and administration periods in the PPI group were LPZ 30 mg , CAM 400 mg , MTZ 500 mg ( LCM group ) . The ranitidine group received RNT 300 mg , CAM 400 mg , MTZ 500 mg ( RCM group ) . The cure rate of H. pylori infection was 88 % in the LCM group ; 95 % CI 79-97 and 92 % in the RCM group ; 95 % CI 84-99 .']\n",
            "10037531: tokens=1, ann_vals=8, match=False\n",
            "  First 5 tokens: ['Xylitol for prevention of acute otitis media .']\n",
            "10052279: tokens=1, ann_vals=350, match=False\n",
            "  First 5 tokens: ['Pre-operative short-term pulmonary rehabilitation for patients of chronic obstructive pulmonary disease undergoing coronary artery bypass graft surgery . The role of pre-operative short-term pulmonary rehabilitation in patients with chronic obstructive pulmonary disease who undergo coronary artery bypass graft surgery has been assessed for the first time prospectively . Forty-five patients posted for coronary artery bypass graft surgery were randomised to receive either short-term pulmonary rehabilitation ( group I ) or no such programme ( group II ) . Patients of both the groups were evenly matched with respect to age , sex , body surface area , duration and severity of chronic obstructive pulmonary disease and coronary artery disease . Normal individuals who evenly matched with the study group were assessed for normal respiratory function parameters . Pre-operative and post-operative peak expiratory flow rate , inspiratory capacity , post-operative ventilation time , post-operative pulmonary complication and hospital stay were determined in both the groups . Peak expiratory flow rate ( 220.0 +/- 12.9 and 324.3 +/- 84.3 in group I , 218.0 +/- 16.4 and 260.5 +/- 35.2 in group II ) and inspiratory capacity ( 844.0 +/- 147.4 and 1100.0 +/- 158.1 in group I , 830.0 +/- 117.4 and 1090 +/- 137 in group II ) were significantly lower before and after surgery respectively in both groups compared to normal values . Even though both groups showed a significant rise in post-operative peak expiratory flow rate and inspiratory capacity after surgery , the post-operative peak expiratory flow rate and inspiratory capacity in group I was significantly higher than in group II . In group I , the post-operative ventilation time ( 24.5 +/- 6.00 hours ) , post-operative complications ( n = 4 ) and hospital stay ( 12.4 +/- 3.6 days ) were significantly lower than in group II ( 35.2 +/- 22.3 hours , n = 11 , 18.8 +/- 6.6 days respectively ) . These data suggest that short-term pulmonary rehabilitation is feasible and effective in improving pulmonary functions before and after surgery and in reducing surgical morbidity and cost of medical care significantly .']\n",
            "10071998: tokens=1, ann_vals=335, match=False\n",
            "  First 5 tokens: ['Anesthesia for in vitro fertilization : the addition of fentanyl to 1.5 % lidocaine . UNLABELLED Ultrasonically guided transvaginal oocyte retrieval is relatively short procedure that is performed on an out-patient basis . The optimal anesthetic technique should allow good surgical anesthesia with minimal side effects , a short recovery time , and , if possible , a high rate of successful pregnancy . Spinal anesthesia is often used in this institution , as well as many others , for this procedure . The addition of fentanyl may be effective for both intraoperative and postoperative pain relief . We assessed the effect of adding fentanyl to 1.5 % lidocaine in women undergoing ultrasonically guided oocyte retrieval . Seventy-eight women were randomized to receive 45 mg of hyperbaric 1.5 % lidocaine with or without 10 microg of fentanyl . Visual analog scale ( VAS ) pain scores were lower in the operating room ( OR ) ( P < 0.05 ) and postanesthesia care unit ( PACU ) ( P < 0.0005 ) for the group that received fentanyl . In addition , the amount of narcotic required in the PACU was less in the fentanyl group ( P < 0.005 ) . There was no difference in VAS scores the evening of or 24 h after the procedure . The amount of analgesics and narcotics required after discharge was the same for both groups . Timed variables , such as time to urination , ambulation , and discharge , were the same for both groups of women . The addition of fentanyl to lidocaine for transvaginal oocyte retrieval results in a more comfortable patient in the OR and PACU . IMPLICATIONS This study demonstrates that when fentanyl is added to a local anesthetic , lidocaine , with spinal anesthesia for egg retrieval procedures , patients are more comfortable during the procedure compared with those who receive lidocaine alone . In addition , the narcotic requirements of patients are less in the postanesthesia care unit .']\n",
            "10073522: tokens=1, ann_vals=12, match=False\n",
            "  First 5 tokens: ['Fertility outcome after systemic methotrexate and laparoscopic salpingostomy for tubal pregnancy .']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import glob\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "base2 = \"EBM-NLP/ebm_nlp_1_00\"\n",
        "ann_base = f\"{base2}/annotations/aggregated/starting_spans\"\n",
        "doc_base = f\"{base2}/documents\"\n",
        "\n",
        "def load_pico_data_fixed(split=\"train\"):\n",
        "    label_map = {\"participants\": \"P\", \"interventions\": \"I\", \"outcomes\": \"O\"}\n",
        "\n",
        "    doc_ids = None\n",
        "    for entity in label_map:\n",
        "        split_path = f\"{ann_base}/{entity}/{split}\"\n",
        "        if not os.path.exists(split_path):\n",
        "            return [], []\n",
        "        ids = set(f.replace(\"_AGGREGATED.ann\", \"\")\n",
        "                  for f in os.listdir(split_path)\n",
        "                  if f.endswith(\".ann\"))\n",
        "        doc_ids = ids if doc_ids is None else doc_ids & ids\n",
        "\n",
        "    all_tokens, all_labels = [], []\n",
        "    skipped = 0\n",
        "\n",
        "    for doc_id in sorted(doc_ids):\n",
        "        tok_file = f\"{doc_base}/{doc_id}.tokens\"\n",
        "        if not os.path.exists(tok_file):\n",
        "            skipped += 1\n",
        "            continue\n",
        "\n",
        "        # FIX: read whole file and split on whitespace\n",
        "        with open(tok_file) as f:\n",
        "            tokens = f.read().split()\n",
        "\n",
        "        pico_labels = {}\n",
        "        valid = True\n",
        "        for entity, letter in label_map.items():\n",
        "            ann_file = f\"{ann_base}/{entity}/{split}/{doc_id}_AGGREGATED.ann\"\n",
        "            with open(ann_file) as f:\n",
        "                vals = [int(x) for x in f.read().strip().split(\",\") if x.strip()]\n",
        "            if len(vals) != len(tokens):\n",
        "                skipped += 1\n",
        "                valid = False\n",
        "                break\n",
        "            pico_labels[letter] = vals\n",
        "\n",
        "        if not valid:\n",
        "            continue\n",
        "\n",
        "        labels = []\n",
        "        for i in range(len(tokens)):\n",
        "            if   pico_labels[\"P\"][i]: labels.append(\"B-P\")\n",
        "            elif pico_labels[\"I\"][i]: labels.append(\"B-I\")\n",
        "            elif pico_labels[\"O\"][i]: labels.append(\"B-O\")\n",
        "            else:                     labels.append(\"O\")\n",
        "\n",
        "        all_tokens.append(tokens)\n",
        "        all_labels.append(labels)\n",
        "\n",
        "    print(f\"Loaded: {len(all_tokens)}, Skipped: {skipped}\")\n",
        "    return all_tokens, all_labels\n",
        "\n",
        "train_tok, train_lab = load_pico_data_fixed(\"train\")\n",
        "train_tok, test_tok, train_lab, test_lab = train_test_split(\n",
        "    train_tok, train_lab, test_size=0.15, random_state=42\n",
        ")\n",
        "train_tok, dev_tok, train_lab, dev_lab = train_test_split(\n",
        "    train_tok, train_lab, test_size=0.1, random_state=42\n",
        ")\n",
        "\n",
        "print(f\"Train: {len(train_tok)} | Dev: {len(dev_tok)} | Test: {len(test_tok)}\")\n",
        "print(f\"Sample tokens: {train_tok[0][:8]}\")\n",
        "print(f\"Sample labels: {train_lab[0][:8]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WYUrqfP-H8nk",
        "outputId": "f8515dbe-9a8e-4fec-f4fe-8f1fa7f31b9c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded: 4802, Skipped: 0\n",
            "Train: 3672 | Dev: 409 | Test: 721\n",
            "Sample tokens: ['Letter', ':', 'Cephaloridine', 'and', 'gentamicin', 'in', 'prophylaxis', 'of']\n",
            "Sample labels: ['B-P', 'B-P', 'B-P', 'B-P', 'B-P', 'B-P', 'B-P', 'B-P']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "full training pipeline:"
      ],
      "metadata": {
        "id": "Y5UJoqQFJF2m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dataset class and tokenization"
      ],
      "metadata": {
        "id": "w59037y3JG4-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import AutoTokenizer, AutoModelForTokenClassification, get_linear_schedule_with_warmup\n",
        "from torch.optim import AdamW\n",
        "from seqeval.metrics import f1_score, precision_score, recall_score\n",
        "from tqdm import tqdm\n",
        "\n",
        "MODEL_NAME = \"microsoft/BiomedNLP-BiomedBERT-base-uncased-abstract\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "\n",
        "label2id = {\"O\": 0, \"B-P\": 1, \"B-I\": 2, \"B-O\": 3}\n",
        "id2label = {v: k for k, v in label2id.items()}\n",
        "NUM_LABELS = len(label2id)\n",
        "\n",
        "class PICODataset(Dataset):\n",
        "    def __init__(self, all_tokens, all_labels, max_len=512):\n",
        "        self.samples = []\n",
        "        for tokens, labels in zip(all_tokens, all_labels):\n",
        "            enc = tokenizer(\n",
        "                tokens,\n",
        "                is_split_into_words=True,\n",
        "                truncation=True,\n",
        "                max_length=max_len,\n",
        "                padding=\"max_length\",\n",
        "                return_tensors=\"pt\"\n",
        "            )\n",
        "            word_ids = enc.word_ids()\n",
        "            label_ids = []\n",
        "            prev_word_id = None\n",
        "            for wid in word_ids:\n",
        "                if wid is None:\n",
        "                    label_ids.append(-100)\n",
        "                elif wid != prev_word_id:\n",
        "                    label_ids.append(label2id[labels[wid]])\n",
        "                else:\n",
        "                    label_ids.append(-100)  # mask subword continuations\n",
        "                prev_word_id = wid\n",
        "\n",
        "            self.samples.append({\n",
        "                \"input_ids\": enc[\"input_ids\"].squeeze(),\n",
        "                \"attention_mask\": enc[\"attention_mask\"].squeeze(),\n",
        "                \"labels\": torch.tensor(label_ids)\n",
        "            })\n",
        "\n",
        "    def __len__(self): return len(self.samples)\n",
        "    def __getitem__(self, idx): return self.samples[idx]\n",
        "\n",
        "print(\"Building datasets...\")\n",
        "train_dataset = PICODataset(train_tok, train_lab)\n",
        "dev_dataset   = PICODataset(dev_tok,   dev_lab)\n",
        "test_dataset  = PICODataset(test_tok,  test_lab)\n",
        "print(f\"Done. Train: {len(train_dataset)}, Dev: {len(dev_dataset)}, Test: {len(test_dataset)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tC-AJbHhJGLE",
        "outputId": "99f7b8f8-b729-4e6d-ce46-aeebe8283336"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py:949: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Building datasets...\n",
            "Done. Train: 3672, Dev: 409, Test: 721\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 16\n",
        "EPOCHS = 5\n",
        "LR = 2e-5\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "dev_loader   = DataLoader(dev_dataset,   batch_size=32)\n",
        "test_loader  = DataLoader(test_dataset,  batch_size=32)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using: {device}\")\n",
        "\n",
        "model = AutoModelForTokenClassification.from_pretrained(\n",
        "    MODEL_NAME, num_labels=NUM_LABELS,\n",
        "    id2label=id2label, label2id=label2id\n",
        ").to(device)\n",
        "\n",
        "optimizer = AdamW(model.parameters(), lr=LR, weight_decay=0.01)\n",
        "total_steps = len(train_loader) * EPOCHS\n",
        "scheduler = get_linear_schedule_with_warmup(\n",
        "    optimizer,\n",
        "    num_warmup_steps=int(0.1 * total_steps),\n",
        "    num_training_steps=total_steps\n",
        ")\n",
        "\n",
        "def evaluate(loader):\n",
        "    model.eval()\n",
        "    all_preds, all_true = [], []\n",
        "    with torch.no_grad():\n",
        "        for batch in loader:\n",
        "            input_ids = batch[\"input_ids\"].to(device)\n",
        "            attention_mask = batch[\"attention_mask\"].to(device)\n",
        "            labels = batch[\"labels\"].to(device)\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "            logits = outputs.logits\n",
        "            preds = torch.argmax(logits, dim=-1)\n",
        "            for pred_seq, true_seq in zip(preds, labels):\n",
        "                p_list, t_list = [], []\n",
        "                for p, t in zip(pred_seq, true_seq):\n",
        "                    if t.item() == -100:\n",
        "                        continue\n",
        "                    p_list.append(id2label[p.item()])\n",
        "                    t_list.append(id2label[t.item()])\n",
        "                all_preds.append(p_list)\n",
        "                all_true.append(t_list)\n",
        "    f1 = f1_score(all_true, all_preds)\n",
        "    p  = precision_score(all_true, all_preds)\n",
        "    r  = recall_score(all_true, all_preds)\n",
        "    return f1, p, r\n",
        "\n",
        "# Training\n",
        "best_dev_f1 = 0\n",
        "for epoch in range(EPOCHS):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}\"):\n",
        "        input_ids = batch[\"input_ids\"].to(device)\n",
        "        attention_mask = batch[\"attention_mask\"].to(device)\n",
        "        labels = batch[\"labels\"].to(device)\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        loss = outputs.loss\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "        optimizer.zero_grad()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    avg_loss = total_loss / len(train_loader)\n",
        "    dev_f1, dev_p, dev_r = evaluate(dev_loader)\n",
        "    print(f\"Epoch {epoch+1} | Loss: {avg_loss:.4f} | Dev F1: {dev_f1:.4f} | P: {dev_p:.4f} | R: {dev_r:.4f}\")\n",
        "\n",
        "    if dev_f1 > best_dev_f1:\n",
        "        best_dev_f1 = dev_f1\n",
        "        torch.save(model.state_dict(), \"best_pico_model.pt\")\n",
        "        print(f\"  ✓ New best saved\")\n",
        "\n",
        "# Final test evaluation\n",
        "model.load_state_dict(torch.load(\"best_pico_model.pt\"))\n",
        "test_f1, test_p, test_r = evaluate(test_loader)\n",
        "print(f\"\\nTest F1: {test_f1:.4f} | P: {test_p:.4f} | R: {test_r:.4f}\")\n",
        "print(f\"Paper target: 73.38\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FnNRB_siJMz7",
        "outputId": "70708e65-b1c2-41e3-f734-614ccbadad0c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForTokenClassification were not initialized from the model checkpoint at microsoft/BiomedNLP-BiomedBERT-base-uncased-abstract and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Epoch 1: 100%|██████████| 230/230 [06:02<00:00,  1.58s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 | Loss: 0.6735 | Dev F1: 0.7239 | P: 0.7546 | R: 0.6956\n",
            "  ✓ New best saved\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2: 100%|██████████| 230/230 [06:03<00:00,  1.58s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2 | Loss: 0.4117 | Dev F1: 0.7335 | P: 0.7406 | R: 0.7265\n",
            "  ✓ New best saved\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3: 100%|██████████| 230/230 [06:03<00:00,  1.58s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3 | Loss: 0.3764 | Dev F1: 0.7252 | P: 0.7567 | R: 0.6962\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4: 100%|██████████| 230/230 [06:03<00:00,  1.58s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4 | Loss: 0.3514 | Dev F1: 0.7328 | P: 0.7449 | R: 0.7211\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5: 100%|██████████| 230/230 [06:02<00:00,  1.58s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5 | Loss: 0.3331 | Dev F1: 0.7315 | P: 0.7584 | R: 0.7064\n",
            "\n",
            "Test F1: 0.7271 | P: 0.7341 | R: 0.7202\n",
            "Paper target: 73.38\n"
          ]
        }
      ]
    }
  ]
}